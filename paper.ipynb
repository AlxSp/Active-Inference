{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A step-by-step tutorial on active inference and its application to empirical data\n",
    "\n",
    "- [Paper](https://www.sciencedirect.com/science/article/pii/S0022249621000973)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from jax import grad\n",
    "from jax import numpy as jnp\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_ = 1e-16\n",
    "eps_ = np.exp(-16) #NOTE: in all their script and experiments, they use this epsilon value. It seems too large, but will use for now.\n",
    "\n",
    "def log(x):\n",
    "    return np.log(x + eps_)\n",
    "\n",
    "def softmax(x):\n",
    "    # stable softmax\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Free Energy Approximation\n",
    "\n",
    "$$ F = \\sum_{s \\in S} q(s) \\ln \\frac{q(s)}{p(o, s)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example from Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = np.array([.5, .5]) # prior p(s)\n",
    "\n",
    "p_o_given_s = np.array([.8, .2]) # likelihood p(o|s)\n",
    "\n",
    "joint_p_o_s =  p_o_given_s * prior # joint p(o, s)\n",
    "joint_p_o_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the exact posterior\n",
    "\n",
    "p_o = joint_p_o_s.sum() # marginal likelihood p(o)\n",
    "\n",
    "p_s_given_o = joint_p_o_s / p_o # posterior p(s|o)\n",
    "p_s_given_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing the variational free energy will approach the true posterior distribution as it the upper bound on the log model evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the approximate posterior\n",
    "\n",
    "# set the initial approximate posterior q(s) to be the prior\n",
    "q_s = prior.copy() # approximate posterior q(s)\n",
    "\n",
    "F = q_s.dot(np.log(q_s / joint_p_o_s)) # compute the KL divergence between the joint and the approximate posterior\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free energy can also be computed with the exact posterior distribution to demonstrate that the variational free energy is an upper bound on the log model evidence and minimization of the variational free energy will lead q(s) to approach the true posterior distribution.\n",
    "\n",
    "$$ F = E_{q(s)} \\ln \\frac{q(s)}{p(s|o)} - \\ln p(o) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = np.asarray(q_s).dot(np.log(q_s / p_s_given_o)) - np.log(p_o)\n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a jax version of the free energy\n",
    "grad_F = grad(lambda q_s: q_s.dot(jnp.log(q_s / joint_p_o_s))) # compute the gradient of the KL divergence with respect to the approximate posterior q(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell ~ 180 times to converge to the exact posterior\n",
    "learning_rate = .1\n",
    "# convert the update log probabilities to probabilities via softmax\n",
    "q_s = softmax(np.log(q_s) - grad_F(q_s) * learning_rate) # compute the gradient of the KL divergence at the initial approximate posterior\n",
    "q_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ F = E_{q(s)} [\\ln q(s) - \\ln p(o,s)] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = q_s.dot(np.log(q_s) - np.log(joint_p_o_s))\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ F = E_{q(s)} [\\ln q(s) - \\ln p(s)] - E_{q(s)}[\\ln p (o|s)] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = q_s.dot(np.log(q_s) - np.log(prior)) - q_s.dot(np.log(p_o_given_s))\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ F = D_{KL} [q(s) || p(s)] - E_{q(s)}[\\ln p (o|s)] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = q_s.dot(np.log(q_s / prior)) - q_s.dot(np.log(p_o_given_s))\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 5 - Static Perception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.array([.5, .5]) # Prior\n",
    "\n",
    "A = np.array([\n",
    "    [.9, .2],\n",
    "    [.1, .8]\n",
    "]) # likelihood p(o|s) \n",
    "\n",
    "o = np.array([1, 0]) # output\n",
    "\n",
    "q_s = softmax(np.log(D) + np.log(A.T.dot(o)))\n",
    "q_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal Message Passing\n",
    "\n",
    "1. Initialize the values of the approximate posteriors q(s_(?,?) ) \n",
    "   for all hidden variables (i.e., all edges) in the graph. \n",
    "2. Fix the value of observed variables (here, o_?).\n",
    "3. Choose an edge (V) corresponding to the hidden variable you want to \n",
    "   infer (here, s_(?,?)).\n",
    "4. Calculate the messages, ?(s_(?,?)), which take on values sent by \n",
    "   each factor node connected to V.\n",
    "5. Pass a message from each connected factor node N to V (often written \n",
    "   as ?_(N?V)). \n",
    "6. Update the approximate posterior represented by V according to the \n",
    "   following rule: q(s_(?,?) )? ? ?(s_(?,?))? ?(s_(?,?)). The arrow \n",
    "   notation here indicates messages from two different factors arriving \n",
    "   at the same edge. \n",
    "    6A. Normalize the product of these messages so that q(s_(?,?) ) \n",
    "        corresponds to a proper probability distribution. \n",
    "    6B. Use this new q(s_(?,?) ) to update the messages sent by \n",
    "        connected factors (i.e., for the next round of message passing).\n",
    "7. Repeat steps 4-6 sequentially for each edge.\n",
    "8. Steps 3-7 are then repeated until the difference between updates \n",
    "   converges to some acceptably low value (i.e., resulting in stable \n",
    "   posterior beliefs for all edges).\n",
    "\n",
    "from [Message_passing_example.m](https://github.com/rssmith33/Active-Inference-Tutorial-Scripts/blob/main/Message_passing_example.m)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "\n",
    "# Fixed observations and message passing steps. Both observations are fixed from the start / already observed. \n",
    "# In full active inference there is another time step variable which acts to sequentially present the observations.\n",
    "\n",
    "# prior p(s) regarding the initial two states\n",
    "D = np.array([.5, .5])\n",
    "\n",
    "# likelihood p(o|s) \n",
    "A = np.array([\n",
    "    [.9, .1],\n",
    "    [.1, .9]\n",
    "])\n",
    "\n",
    "# state to state transition probability matrix p(s_ τ+1 |s_ τ)\n",
    "B = np.array([\n",
    "    [1, 0],\n",
    "    [0, 1]\n",
    "])\n",
    "\n",
    "# transpose the transition matrices and normalize the columns for future message passing\n",
    "# Note: this technically not necessary for this example as B.T is already normalized, but it is included for completeness\n",
    "B_T = [\n",
    "    b.T / b.T.sum(axis =0) for b in B \n",
    "]\n",
    "\n",
    "# fixed observations o_ τ\n",
    "# Fix the observations at each time step (Step 2)\n",
    "o_arr = np.array([\n",
    "    [1, 0],\n",
    "    [1, 0]\n",
    "])\n",
    "\n",
    "time_steps = len(o_arr) # number of time steps\n",
    "\n",
    "num_iter = 16 # number of iterations of message passing\n",
    "\n",
    "# Initialize approximate posteriors q(s) at each time step (Step 1)\n",
    "qs_arr = np.ones((time_steps, len(D))) / len(D) # array of approximate posteriors q(s) at each time step\n",
    "\n",
    "# Initialize history of approximate posteriors q(s) for each iteration and time step (This variable is only used for visualization)\n",
    "qs_history = np.zeros((num_iter, time_steps, len(D)))\n",
    "\n",
    "# Iterate a set number of times or until convergence (Step 8) \n",
    "for i in range(num_iter):\n",
    "    # For each edge (hidden state) (Step 7)\n",
    "    for tt in range(time_steps):\n",
    "        # get the log of the approximate posterior q(s) at this time step (Step 3)\n",
    "        q_s = np.log(qs_arr[tt])\n",
    "\n",
    "        # get the message sent from the past (Step 4: Message 1) \n",
    "        if tt == 0: # if this is the first time step we use the prior\n",
    "            log_B_past = np.log(D)\n",
    "        else: # otherwise we compute the belief of the current state based on the previous state and the transition matrix\n",
    "            log_B_past = np.log(B @ qs_arr[tt - 1])\n",
    "\n",
    "        # get the message sent from the future (Step 4: Message 2)\n",
    "        if tt == time_steps - 1: # if this is the last time step we use a message of zero (no future states)\n",
    "            log_B_future = 0\n",
    "        else: # otherwise we compute the belief of the current state based on the future state and the transition matrix\n",
    "            log_B_future = np.log(B_T @ qs_arr[tt + 1])\n",
    "\n",
    "        # get the likelihood of the state given the observation (Step 4: Message 3)\n",
    "        log_Ao = np.log(A.T @ o_arr[tt])\n",
    "\n",
    "        # Pass messages and update the posterior (Step 5-6)\n",
    "        # Since all terms are in log space, this is addition instead of\n",
    "        # multiplication. This corresponds to  equation 16 in the main\n",
    "        # text (within the softmax)\n",
    "        q_s = .5 * (log_B_past + log_B_future) + log_Ao\n",
    "\n",
    "        # normalize the posterior (Step 6A)\n",
    "        qs_arr[tt] = softmax(q_s)\n",
    "\n",
    "        qs_history[i, tt] = qs_arr[tt]\n",
    "\n",
    "qs_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 1: Posterior over states \n",
    "\n",
    "plt.matshow(qs_arr.T, vmin = 0.0, vmax = 1.0, cmap='binary')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Approximate Posterior $q(s)$')\n",
    "\n",
    "qs_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack([np.array([[D] * 2]), qs_history]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Note: the initial prior is not always added to the history when plotting\n",
    "qs_history_with_priors = np.vstack([np.array([[D] * 2]), qs_history])\n",
    "\n",
    "plt.plot(qs_history_with_priors.reshape(-1, 4))\n",
    "plt.ylabel('Approximate Posterior Probability, $q(s_{tau})$')\n",
    "plt.xlabel('Message Passing Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2\n",
    "\n",
    "# prior p(s) regarding the initial two states\n",
    "D = np.array([.5, .5])\n",
    "\n",
    "# likelihood p(o|s) \n",
    "A = np.array([\n",
    "    [.9, .1],\n",
    "    [.1, .9]\n",
    "])\n",
    "\n",
    "# state to state transition probability matrix p(s_ τ+1 |s_ τ)\n",
    "B = np.array([\n",
    "    [1, 0],\n",
    "    [0, 1]\n",
    "])\n",
    "\n",
    "# transpose the transition matrices and normalize the columns for future message passing\n",
    "# Note: this technically not necessary for this example as B.T is already normalized, but it is included for completeness\n",
    "B_T = [\n",
    "    b.T / b.T.sum(axis =0) for b in B \n",
    "]\n",
    "\n",
    "# In the original Message_passing_example.m code the sequential observations are defined as a matrix; (τ, t)\n",
    "# this symbolizes that each τ can see all observations up to τ.\n",
    "# Hence the second observation of the first τ is [0, 0] (not observed)\n",
    "# o_arr = np.array([\n",
    "#     [\n",
    "#         [1, 0],\n",
    "#         [0, 0]\n",
    "#     ],\n",
    "#     [\n",
    "#         [1, 0],\n",
    "#         [1, 0]\n",
    "#     ]\n",
    "# ])\n",
    "# For simplicity, we will instead just check in each iteration if τ is less than or equal to t and then use the observation at that time step.\n",
    "# Otherwise, we will set log_Ao to zero.\n",
    "\n",
    "# fixed observations o_ τ\n",
    "# Fix the observations at each time step (Step 2)\n",
    "o_arr = np.array([\n",
    "    [1, 0],\n",
    "    [1, 0]\n",
    "])\n",
    "\n",
    "time_steps = len(o_arr) # number of time steps\n",
    "\n",
    "num_iter = 16 # number of iterations of message passing\n",
    "\n",
    "# Initialize approximate posteriors q(s) at each time step (Step 1)\n",
    "\n",
    "qs_arr = np.ones((time_steps, len(D))) / len(D) # array of approximate posteriors q(s) at each time step\n",
    "\n",
    "# Initialize history of approximate posteriors q(s) for each iteration and time step (This variable is only used for visualization)\n",
    "qs_history = np.zeros((time_steps, num_iter, time_steps, len(D))) \n",
    "# Initialize history of errors for each iteration and time step (This variable is only used for visualization)\n",
    "err_history = np.zeros((time_steps, num_iter, time_steps, len(D))) \n",
    "\n",
    "# for each time step (over all observations)\n",
    "for t in range(time_steps):\n",
    "    # for each factor (light blue shapes, light green shapes)\n",
    "    for i in range(num_iter):\n",
    "        # for each time step (over all observations)\n",
    "        for tt in range(time_steps):\n",
    "\n",
    "            # get the log of the approximate posterior q(s) at this time step (Step 3)\n",
    "            v = log(qs_arr[tt])\n",
    "\n",
    "            # get the message sent from the past (Step 4: Message 1) \n",
    "            if tt == 0: # if this is the first time step we use the prior\n",
    "                log_B_past = log(D)\n",
    "            else: # otherwise we compute the belief of the current state based on the previous state and the transition matrix\n",
    "                log_B_past = log(B @ qs_arr[tt - 1])\n",
    "\n",
    "            # get the message sent from the future (Step 4: Message 2)\n",
    "            if tt == time_steps - 1: # if this is the last time step we use a message of zero (no future states)\n",
    "                log_B_future = 0\n",
    "            else: # otherwise we compute the belief of the current state based on the future state and the transition matrix\n",
    "                log_B_future = log(B_T @ qs_arr[tt + 1])\n",
    "\n",
    "            # get the likelihood of the state given the observation (Step 4: Message 3)\n",
    "            if tt <= t: # if the observation has been observed\n",
    "                log_Ao = log(A.T  @ o_arr[tt] )\n",
    "            else: # if the observation has not been observed\n",
    "                log_Ao = 0\n",
    "    \n",
    "            err = 0.5 * (log_B_past + log_B_future) + log_Ao - v\n",
    "\n",
    "            v += err\n",
    "\n",
    "            qs_arr[tt] = softmax(v)\n",
    "\n",
    "            err_history[t, i, tt] = err\n",
    "            qs_history[t, i, tt] = qs_arr[tt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2: Posterior over states \n",
    "\n",
    "plt.matshow(qs_arr.T, vmin = 0.0, vmax = 1.0, cmap='binary')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Approximate Posterior $q(s)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the initial prior is not always added to the history when plotting\n",
    "full_beliefs = []\n",
    "full_beliefs.append(np.array([D] * 2).flatten())\n",
    "for t in range(time_steps):\n",
    "    for i in range(num_iter):\n",
    "        full_beliefs.append(qs_history[t][i].flatten())\n",
    "full_beliefs = np.asarray(full_beliefs)\n",
    "plt.plot(full_beliefs)\n",
    "\n",
    "plt.ylabel('Approximate Posterior Probability, $q(s_{tau})$')\n",
    "plt.xlabel('Message Passing Iterations')\n",
    "plt.title(\"Firing Rates (traces)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = 2\n",
    "epochs = 2\n",
    "\n",
    "event_related_potentials = []\n",
    "\n",
    "for tau in range(4):\n",
    "    epoch_gradients = []\n",
    "    for i in range(epochs):\n",
    "            # since we attach the original prior to the beginning of the array, we need to skip the first element\n",
    "            start_offset = i * num_iter\n",
    "            end_offset = i * num_iter + num_iter\n",
    "            epoch_gradients.append(np.gradient(full_beliefs[1:][start_offset:end_offset, tau]))\n",
    "\n",
    "    event_related_potentials.append(np.concatenate(epoch_gradients))\n",
    "\n",
    "event_related_potentials = np.asarray(event_related_potentials)\n",
    "\n",
    "event_related_potentials = np.hstack((np.zeros((4, 1)), event_related_potentials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(event_related_potentials.T)\n",
    "plt.xlabel('Message Passing Iterations')\n",
    "plt.ylabel('Response')\n",
    "plt.title(\"Event-Related Potentials (ERPs) for each state\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Prediction Errors\n",
    "\n",
    "επ,τ ←\n",
    "1(\n",
    "2\n",
    "ln Bπ,τ −1 sπ,τ −1 + ln B†π,τ sπ,τ +1\n",
    "(\n",
    "− ln sπ,τ\n",
    ")\n",
    "(\n",
    "))\n",
    "+ ln AT oτ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [.8, .4],\n",
    "    [.2, .6]\n",
    "])\n",
    "\n",
    "B_past = np.array([\n",
    "    [.9, .2],\n",
    "    [.1, .8]\n",
    "])\n",
    "\n",
    "B_current = np.array([\n",
    "    [.2, .3],\n",
    "    [.8, .7]\n",
    "])\n",
    "\n",
    "# NOTE: in shape_patterns.ipynb the normalization is done via division by the sum of the columns instead of softmax\n",
    "B_T = softmax(B_current.T)\n",
    "\n",
    "o = np.array([1, 0])\n",
    "\n",
    "q_s = np.array([.5, .5])\n",
    "\n",
    "q_s_past, q_s_future = q_s, q_s\n",
    "\n",
    "v = log(q_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = 0.5 * (log(B_past @ q_s) + log(B_T @ q_s)) + log(A.T @ o) - v\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = v + err\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_s = softmax(v)\n",
    "q_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_past @ q_s_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(B_current.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome Prediction Errors\n",
    "\n",
    "ςπ,τ = Asπ,τ · ln Asπ,τ − ln Cτ − diag AT ln A · sπ,τ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [.9, .1],\n",
    "    [.1, .9]\n",
    "])\n",
    "\n",
    "C = np.array([1, 0])\n",
    "\n",
    "qs_p1 = np.array([.9, .1])\n",
    "\n",
    "qs_p2 = np.array([.5, .5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_qs_p1 = A @ qs_p1\n",
    "A_qs_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_qs_p1 @ (log(A_qs_p1) - log(C)) # expected difference between preferred outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_q_s_2 = A @ qs_p2\n",
    "A_q_s_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_q_s_2 @ (log(A_q_s_2) - log(C)) # expected difference between preferred outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [.4, .2],\n",
    "    [.6, .8]\n",
    "])\n",
    "\n",
    "qs_p1 = np.array([.9, .1])\n",
    "qs_p2 = np.array([.1, .9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-1 * np.diag(A.T @ log(A)) @ qs_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-1 * np.diag(A.T @ log(A)) @ qs_p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building specific task models\n",
    "\n",
    "#FIXME: copied\n",
    "In the beginning of the explore–exploit task, the participant\n",
    "is told that on each trial one machine will tend to pay out more\n",
    "often, but they will not know which one. They are also told that\n",
    "the better machine will not always be the same on each trial.\n",
    "They can choose to select one right away and possibly win $4.\n",
    "Or they can choose to press a button that gives them a hint about\n",
    "which slot machine is better on that trial. However, if they choose\n",
    "to take the hint, they can only win $2 if they pick the correct\n",
    "machine. Over many trials, the participant can learn which slot\n",
    "machine tends to pay out more often and either make safe or\n",
    "risky choices (i.e., take the hint or not).\n",
    "\n",
    "## Capital letters stand for the generative process and lower case letters stand for the generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative process priors\n",
    "\n",
    "# context: both machines are equally likely to have the better outcome\n",
    "D_1 = np.array([.5, .5]) # numbers in paper \n",
    "# D_1 = np.array([1, 0]) # numbers in code\n",
    "\n",
    "# choices:\n",
    "# 1. start\n",
    "# 2. hint\n",
    "# 3. choose left/0\n",
    "# 4. choose right/1\n",
    "D_2 = np.array([1, 0, 0, 0])\n",
    "\n",
    "D = [D_1, D_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative model priors\n",
    "\n",
    "d_1 = np.array([.25, .25])\n",
    "\n",
    "d_2 = np.array([1, 0, 0, 0])\n",
    "\n",
    "d = [d_1, d_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_1 represents the observation likelihoods corresponding to the hint given a state \n",
    "# there is a outcome likelihood for each possible choice (start, hint, left, right)\n",
    "# for each likelihood matrix, the rows correspond to the observations, (no hint, left is better, right is better)\n",
    "# and the columns correspond to the number of context (left is better, right is better)\n",
    "\n",
    "phA = 1 # probability of accuracy of the hint\n",
    "\n",
    "A_1 = np.array([\n",
    "    # start\n",
    "    [\n",
    "        [1, 1], # no hint\n",
    "        [0, 0], # left is better\n",
    "        [0, 0], # right is better\n",
    "    ],\n",
    "    # hint\n",
    "    [\n",
    "        [0, 0], # no hint\n",
    "        [phA, 1 - phA], # left is better\n",
    "        [1 - phA, phA], # right is better\n",
    "    ],\n",
    "    # choose left\n",
    "    [\n",
    "        [1, 1], # no hint\n",
    "        [0, 0], # left is better\n",
    "        [0, 0], # right is better\n",
    "    ],\n",
    "    # choose right\n",
    "    [\n",
    "        [1, 1], # no hint\n",
    "        [0, 0], # left is better\n",
    "        [0, 0], # right is better\n",
    "    ],\n",
    "])\n",
    "\n",
    "# A_2 represents the observation likelihoods corresponding to the win/lose outcomes given a state\n",
    "# there is a outcome likelihood for each possible choice (start, hint, left, right)\n",
    "# for each likelihood matrix, the rows correspond to the observations, (undetermined, win, loss)\n",
    "# and the columns correspond to the number of context (left is better, right is better)\n",
    "\n",
    "pWin = .8 # probability of winning\n",
    "\n",
    "A_2 = np.array([\n",
    "    # start\n",
    "    [\n",
    "        [1, 1], # undetermined\n",
    "        [0, 0], # loss\n",
    "        [0, 0], # win\n",
    "    ],\n",
    "    # hint\n",
    "    [\n",
    "        [1, 1], # undetermined\n",
    "        [0, 0], # loss\n",
    "        [0, 0], # win\n",
    "    ],\n",
    "    # choose left\n",
    "    [\n",
    "        [0, 0], # undetermined\n",
    "        [1 - pWin, pWin], # loss\n",
    "        [pWin, 1 - pWin], # win\n",
    "    ],\n",
    "    # choose right\n",
    "    [\n",
    "        [0, 0], # undetermined\n",
    "        [pWin, 1 - pWin], # loss\n",
    "        [1 - pWin, pWin], # win\n",
    "    ],\n",
    "])\n",
    "\n",
    "# A_3 represents the mapping between the behavior states and observed behaviors\n",
    "\n",
    "A_3 = np.array([\n",
    "    [\n",
    "        [1, 1],\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "    ],\n",
    "    [\n",
    "        [0, 0],\n",
    "        [1, 1],\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "    ],\n",
    "    [\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [1, 1],\n",
    "        [0, 0],\n",
    "    ],\n",
    "    [\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [1, 1],\n",
    "    ],\n",
    "])\n",
    "\n",
    "A = [A_1, A_2, A_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_1 represents the transition probabilities between the context states. Since the agent can not change the context, the transition matrix is the identity matrix\n",
    "\n",
    "B_1 = np.array([\n",
    "    [1, 0], # left is better\n",
    "    [0, 1], # right is better\n",
    "])\n",
    "\n",
    "# Note: technically only one matrix is required but to make python operations easier, we will use two matrices\n",
    "B_1 = np.array([\n",
    "    [\n",
    "        [1, 0], # left is better\n",
    "        [0, 1], # right is better\n",
    "    ],\n",
    "    [\n",
    "        [1, 0], # left is better\n",
    "        [0, 1], # right is better\n",
    "    ],\n",
    "])\n",
    "\n",
    "B_2 = np.array([\n",
    "    # move to the start state from any other state\n",
    "    [\n",
    "        [1, 1, 1, 1], # start\n",
    "        [0, 0, 0, 0], # hint\n",
    "        [0, 0, 0, 0], # choose left\n",
    "        [0, 0, 0, 0], # choose right\n",
    "    ],\n",
    "    # move to the hint state from any other state\n",
    "    [\n",
    "        [0, 0, 0, 0], # start\n",
    "        [1, 1, 1, 1], # hint\n",
    "        [0, 0, 0, 0], # choose left\n",
    "        [0, 0, 0, 0], # choose right\n",
    "    ],\n",
    "    # move to the choose left state from any other state\n",
    "    [\n",
    "        [0, 0, 0, 0], # start\n",
    "        [0, 0, 0, 0], # hint\n",
    "        [1, 1, 1, 1], # choose left\n",
    "        [0, 0, 0, 0], # choose right\n",
    "    ],\n",
    "    # move to the choose right state from any other state\n",
    "    [\n",
    "        [0, 0, 0, 0], # start\n",
    "        [0, 0, 0, 0], # hint\n",
    "        [0, 0, 0, 0], # choose left\n",
    "        [1, 1, 1, 1], # choose right\n",
    "    ],\n",
    "])\n",
    "\n",
    "# B_2 = np.ones((4, 4)) / 4\n",
    "\n",
    "B = [B_1, B_2]\n",
    "\n",
    "# B_1.T / B_1.T.sum(axis =0)\n",
    "B_future = [b.sum(axis = 0).T / b.sum(axis = 0).T.sum(axis = 0) for b in B]\n",
    "\n",
    "# B_2_f = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_1 = np.array([\n",
    "    [\n",
    "        [1, 0], # left is better\n",
    "        [0, 1], # right is better\n",
    "    ],\n",
    "    [\n",
    "        [1, 0], # left is better\n",
    "        [0, 1], # right is better\n",
    "    ],\n",
    "])\n",
    "\n",
    "B_1.sum(axis = 0).T / B_1.sum(axis = 0).T.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_2.sum(axis = 0).T / B_2.sum(axis = 0).T.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1., 1., 1., 1.],\n",
    "         [0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0.]]) @ np.array([1.00000000e+00, 8.27037108e-18, 8.27037108e-18, 8.27037108e-18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_2.sum(axis = 0).T / B_2.sum(axis = 0).T.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_1.sum(axis = 1).T / B_1.sum(axis = 1).T.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rows are observations, columns indicate time points\n",
    "\n",
    "# no preference for observing a hint or not\n",
    "\n",
    "C_1 = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0]\n",
    "])\n",
    "\n",
    "# Preference for observing a win and not a loss\n",
    "\n",
    "C_2 = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, -1, -1],\n",
    "    [0, 4, 2]\n",
    "])\n",
    "\n",
    "# no preference for observing any action\n",
    "\n",
    "C_3 = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "])\n",
    "\n",
    "C = [C_1, C_2, C_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(softmax(C_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allowable policies\n",
    "\n",
    "U = np.array([\n",
    "    [\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [0, 2],\n",
    "        [0, 3]\n",
    "    ]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows correspond to time points, columns correspond to context states\n",
    "\n",
    "V = np.array([\n",
    "    [\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "        [0, 2],\n",
    "        [0, 3]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0],\n",
    "        [0, 2],\n",
    "        [0, 3],\n",
    "        [0, 0],\n",
    "        [0, 0]\n",
    "    ]\n",
    "])\n",
    "\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.5 # learning rate\n",
    "omega = 1 # forgetting rate\n",
    "beta = 1 # expected precision of expected free energy (G) over policies. (a positive value, with higher values indicating lower expected precision).\n",
    "alpha = 32 # An 'inverse temperature' or 'action precision' parameter that controls how much randomness there is when selecting actions\n",
    "erp = 1 # degree of belief resetting at each time point in a trial when simulating neural responses\n",
    "tau = 12 # time constant for evidence accumulation.\n",
    "zeta = 3 # Occam window policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this function they use a smaller epsilon value for the log function\n",
    "def spm_log(x):\n",
    "    return np.log(x + 1e-16)\n",
    "\n",
    "def multi_dot(a, b, dim):\n",
    "    if dim == 0:\n",
    "        return np.sum(a @ b[1].reshape(-1, 1), axis = 1)\n",
    "    elif dim == 1:\n",
    "        return np.sum(a * b[0].reshape(-1, 1), axis = 0)# np.sum(a @ b[0].reshape(-1, 1), axis = dim)\n",
    "    raise ValueError(\"dim must be 0 or 1\")\n",
    "\n",
    "def multi_dot_no_dim(a, b):\n",
    "    _X = copy.deepcopy(a)\n",
    "    DIM = np.arange(len(b)) + _X.ndim - len(b)\n",
    "\n",
    "    for i in range(len(b)):\n",
    "        # print(i)\n",
    "        s = np.ones(_X.ndim)\n",
    "        s[DIM[i]] = len(b[i])\n",
    "        _X = (_X * b[i].reshape(s.astype(int)))\n",
    "        \n",
    "        _X = _X.sum(axis = i + 1, keepdims = True)\n",
    "\n",
    "    return _X.squeeze()\n",
    "\n",
    "def sum_probs(prob_array):\n",
    "    # FIXME: further understand this fn. normalisation of a probability transition matrix (columns)\n",
    "    s = prob_array + 1e-16\n",
    "    s = (1 / np.sum(s, axis=0) - 1/ s) / 2\n",
    "    return s\n",
    "\n",
    "def normalize(x):\n",
    "    x = x / x.sum(axis = 0)\n",
    "    x[np.isnan(x)] = 1/x.shape[0]\n",
    "    return x\n",
    "\n",
    "# MDP_VB_X_TUTORIAL defined & initialized\n",
    "T = 3\n",
    "\n",
    "s = np.zeros((2, 3))\n",
    "o = np.zeros((3, 3))\n",
    "n = np.zeros((3, 3))\n",
    "\n",
    "# outside variables\n",
    "Ns = [2, 4]\n",
    "Nu = 4#[1, 4]\n",
    "Np = 5\n",
    "No = [3, 3, 4]\n",
    "Ni = 16\n",
    "Ng = 3\n",
    "Nf = 2\n",
    "M = [1, 1, 1]\n",
    "S = 3\n",
    "p = np.arange(Np) #5\n",
    "\n",
    "\n",
    "#FIXME: can be replaced with capital U?\n",
    "# u = np.zeros((T, 1))\n",
    "u = []\n",
    "\n",
    "F_hist = np.zeros((T, Np))\n",
    "Q_hist = np.zeros((T, Np))\n",
    "H_hist = np.zeros(T)\n",
    "\n",
    "qE = spm_log(np.ones(5) / 5)\n",
    "\n",
    "qb = beta # rate parameters\n",
    "w = np.array([1/qb] * T) # posterior precision (policy)\n",
    "\n",
    "pD = copy.deepcopy(d)\n",
    "wD = [sum_probs(_d) for _d in d]\n",
    "\n",
    "# X = copy.deepcopy(D) # TODO: maybe have to be expanded by time points\n",
    "\n",
    "X = [np.zeros((T, *_d.shape)) for _d in D]\n",
    "\n",
    "P = np.zeros((T, Nu))\n",
    "\n",
    "for i, _d in enumerate(D):\n",
    "    X[i][0] = _d\n",
    "\n",
    "\n",
    "x = [np.ones((ns, T, Np)) / ns for ns in Ns]\n",
    "\n",
    "for k in range(Np):\n",
    "    for f in range(Nf):\n",
    "        x[f][:, 0, k] = D[f]\n",
    "\n",
    "xn = [np.zeros((Ni, ns, S, T, Np)) + 1/ns for ns in Ns]\n",
    "vn = [np.zeros((Ni, ns, S, T, Np)) for ns in Ns]\n",
    "\n",
    "wn = np.zeros(T * Ni)\n",
    "un = np.zeros((T * Ni, Np))\n",
    "ext_u = np.zeros((T, Np))\n",
    "\n",
    "ext_A = []\n",
    "for g in range(Ng):\n",
    "    ext_A.append(normalize(np.transpose(A[g], axes=(1, 2, 0))))\n",
    "\n",
    "ext_C = [spm_log(softmax(c)) for c in C] \n",
    "# transition probabilities\n",
    "\n",
    "\n",
    "\n",
    "# variables initialized and used in the loop\n",
    "xqq = [np.zeros_like(_d) for _d in D]\n",
    "xq = [np.zeros_like(_d) for _d in D]\n",
    "\n",
    "# may have to be expanded to fit factors\n",
    "L = np.ones((T, 2, 4))\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# belief updating over successive time points\n",
    "for t in range(T):\n",
    "\n",
    "    # sample state, if not specified\n",
    "    for f in range(Nf):\n",
    "\n",
    "        if s[f, t] == 0:\n",
    "    \n",
    "            if t > 0:\n",
    "                #TODO: check if out put is correct\n",
    "                ps = B[f][u[t -1][f]][:,int(s[f, t - 1])] #B[f][int(s[f, t - 1])][u[t -1][f]]\n",
    "            else:\n",
    "                \n",
    "                ps = D[f] / D[f].sum() # ensure ps is normalized\n",
    "            # sample state\n",
    "            s[f, t] = np.argmax(np.random.rand() < np.cumsum(ps)) # FIXME: check if argmax is correct\n",
    "\n",
    "    # posterior predictive density over hidden (external) states\n",
    "    for f in range(Nf):\n",
    "        # under selected action\n",
    "        if t > 0:\n",
    "            xqq[f] = B[f][u[t -1][f]] @ X[f][t - 1]\n",
    "        else:\n",
    "            xqq[f] = X[f][t]\n",
    "        # Bayesian model average (xq)\n",
    "        xq[f] = X[f][t]\n",
    "\n",
    "    # sample outcome if not specified\n",
    "    for g in range(Ng):\n",
    "        # if outcome not specified\n",
    "        if not o[g, t]:\n",
    "            if n[g, t]: # outcome is generated by model n if not 0\n",
    "                pass\n",
    "            else: # or sampled from likelihood given hidden states\n",
    "                ind = s[:, t]\n",
    "                # po = A[g][int(ind[0]), :, int(ind[1])]\n",
    "                po = A[g][int(ind[1]), :, int(ind[0])]\n",
    "                o[g, t] = np.argmax(np.random.rand() < np.cumsum(po))\n",
    "                pass\n",
    "\n",
    "    O = [] #TODO: May initialize at beginning for all time points\n",
    "    # get outcome likelihoods\n",
    "    for g in range(Ng):\n",
    "        # specified as the sampled outcome\n",
    "        # TODO: this most likely should be the likelihood of an outcome (matlab using index 1 can be misleading of how their function accomplishes this)\n",
    "        os = np.zeros(No[g])\n",
    "        os[int(o[g, t])] = 1\n",
    "        O.append(os)\n",
    "\n",
    "    # likelihood of hidden states\n",
    "    for g in range(Ng):\n",
    "        L[t] *= np.tensordot(np.transpose(A[g], axes=(2, 0, 1)), O[g], axes=([2], [0]))\n",
    "\n",
    "    # TODO: remove and find a better way to handle this\n",
    "    if t == 2:\n",
    "        # swap the first and second row\n",
    "        L[t] = L[t][[1, 0]]\n",
    "\n",
    "    # eliminate unlikely policies\n",
    "    if len(u) - 1 < t  and t > 0: # TODO: len u has to be changed (should check if U is defined)\n",
    "        F = np.log(ext_u[t - 1]) \n",
    "        p = p[(F - np.max(F)) > -zeta]\n",
    "\n",
    "    for f in range(Nf):\n",
    "        x[f] = softmax(spm_log(x[f]) / erp)\n",
    "\n",
    "    S = V.shape[0] + 1 # horizon\n",
    "\n",
    "    R = S\n",
    "\n",
    "    F = np.zeros(Np)\n",
    "    for k in p: # loop over plausible policies\n",
    "        dF = 1 # reset criterion for this policy\n",
    "        for i in range(Ni): # iterate belief updates\n",
    "            F[k] = 0  # reset free energy for this policy\n",
    "            for j in range(S): # loop over future time points\n",
    "                # current posterior over outcome factors\n",
    "                if j <= t:\n",
    "                    for f in range(Nf):\n",
    "                        xq[f] = x[f][:, j, k]\n",
    "\n",
    "                for f in range(Nf):\n",
    "                    # hidden states for this time and policy\n",
    "                    sx = x[f][:, j, k]\n",
    "                    qL = np.zeros(Ns[f])\n",
    "                    v = np.zeros(Ns[f])\n",
    "\n",
    "                    #if f == 0:print(sx, t, k, j, f) # print(sx, t, k, f\"{i:<2}\", j, f)\n",
    "                    # t == 2 and k ==1 and i ==2 and j == 0 and f == 0\n",
    "                    # t == 3 && k == 2 && i == 3 && j == 1 && f == 1\n",
    "                        \n",
    "                    # evaluate free energy and gradients\n",
    "                    if dF > np.exp(-8) or i > 3: \n",
    "                        # marginal likelihood over outcome factors\n",
    "                        if j <= t:\n",
    "                            # FIXME: why does current f dimension get removed?\n",
    "                            # qL = xq[abs(1 - f)] @ L[j].T \n",
    "                            qL = multi_dot(L[j], xq, f)\n",
    "                            qL = spm_log(qL)\n",
    "\n",
    "                        # entropy\n",
    "                        qx = spm_log(sx) # FIXME: qx values are not the same as in the matlab code\n",
    "\n",
    "                        # empirical priors (forward messages)\n",
    "                        if j < 1:\n",
    "                            px = spm_log(D[f])\n",
    "                            v = v + px + qL - qx\n",
    "                        else:\n",
    "                            #TODO: investigate why B and B_future are defined as they are\n",
    "                            px = spm_log(B[f][V[j - 1, k, f]] @ x[f][:, j-1, k])\n",
    "                            # px = np.dot(spm_log(sB[f][:, :,V[j-1, k, f]]), x[f][:, j-1, k])\n",
    "                            v = v + px + qL - qx\n",
    "\n",
    "                        # empirical priors (backward messages)\n",
    "                        if j < R - 1:\n",
    "                            px = spm_log(B_future[f].T @ x[f][:, j+1, k])\n",
    "                            v = v + px + qL - qx\n",
    "\n",
    "                        # (negative) free energy\n",
    "                        if j == 0 or j == S - 1:\n",
    "                            F[k] = F[k] + np.dot(sx.T, v * 0.5)\n",
    "                        else:\n",
    "                            F[k] = F[k] + np.dot(sx.T, v * 0.5 - (Nf - 1) * qL / Nf)\n",
    "\n",
    "                        # update posterior\n",
    "                        v = v - np.mean(v)\n",
    "                        sx = softmax(qx + v / tau)\n",
    "                    \n",
    "                    else:\n",
    "                        F[k] = G[k]\n",
    "\n",
    "                    x[f][:, j, k] = sx\n",
    "                    xq[f] = sx\n",
    "                    xn[f][i, :, j, t, k] = sx\n",
    "                    vn[f][i, :, j, t, k] = v\n",
    "\n",
    "            if i > 0:\n",
    "                dF = F[k] - G[k]\n",
    "            \n",
    "            G = F.copy()\n",
    "\n",
    "    # accumulate expected free energy over policies (Q)\n",
    "    pu = 1 # empirical prior\n",
    "    qu = 1 # posterior\n",
    "    Q = np.zeros(Np) # expected free energy over policies\n",
    "\n",
    "    if Np > 0: #FIXME: check if this should be 1\n",
    "        for k in p:\n",
    "            # bayesian surprise about initial conditions\n",
    "            for f in range(Nf):\n",
    "                Q[k] = Q[k] - wD[f] @ x[f][:, 0, k] # FIXME: should be spm_dot\n",
    "\n",
    "            pass\n",
    "\n",
    "            for j in range(t, S):\n",
    "                # get expected states for this policy and time point\n",
    "                for f in range(Nf): # TODO: check if required\n",
    "                    xq[f] = x[f][:, j, k]\n",
    "\n",
    "                # (negative) expected free energy\n",
    "\n",
    "                # bayesian surprise about states\n",
    "                _qx = xq[0].reshape(-1, 1) * xq[1] \n",
    "\n",
    "                #>> compute G\n",
    "                G = 0 \n",
    "                qo = 0 # Fixme qo should be calculated with spm dot\n",
    "                # for i in np.where(_qx > np.exp(-16))[0]:\n",
    "                for r_i, c_i in zip(*np.where(_qx > np.exp(-16))):\n",
    "                    po = 1\n",
    "                    for g in range(len(ext_A)):\n",
    "                        # kx = np.transpose(ext_A[g], axes=(2, 0, 1))[0, ..., i]\n",
    "                        kx = ext_A[g][..., r_i, c_i]\n",
    "                        reshape_last_dim = [1] * (po.ndim) if isinstance(po, np.ndarray) else []\n",
    "                        reshape_arr = [-1] + reshape_last_dim\n",
    "                        po = po * kx.reshape(reshape_arr)\n",
    "\n",
    "                    po = po.flatten()\n",
    "                    qo = qo + _qx[r_i][c_i] * po\n",
    "                    G = G + _qx[r_i][c_i] * po @ np.log(po + np.exp(-16))  \n",
    "\n",
    "                G = G - qo @ np.log(qo + np.exp(-16))\n",
    "                # << Compute G\n",
    "                Q[k] = Q[k] + G\n",
    "\n",
    "\n",
    "\n",
    "                for g in range(Ng):\n",
    "                    qo = multi_dot_no_dim(ext_A[g], xq)\n",
    "\n",
    "                    Q[k] =  Q[k] + qo.flatten() @ spm_log(softmax(C[g]))[:,j].reshape(-1, 1)\n",
    "\n",
    "                pass\n",
    "\n",
    "    # previous expected precision\n",
    "    if t > 0:\n",
    "        w[t] = w[t - 1] # FIXME: w[t] not the same to matlab code\n",
    "\n",
    "    for i in range(Ni):\n",
    "        qu = softmax(qE[p] + w[t] * Q[p] + F[p]) #FIXME: tiny difference in outcome of Q\n",
    "        pu = softmax(qE[p] + w[t] * Q[p])\n",
    "\n",
    "        # precision (w) with free energy gradients (v = -dF/dw)\n",
    "        eg = (qu - pu) @ Q[p]\n",
    "        dFdg = qb - beta + eg\n",
    "        qb = qb - dFdg / 2\n",
    "        w[t] = 1/qb\n",
    "\n",
    "        # simulated dopamine responses (expected precision)\n",
    "        _n = t * Ni + i #TODO: originally was t - 1\n",
    "        wn[_n] = w[t]\n",
    "        un[_n][p] = qu\n",
    "        ext_u[t][p] = qu\n",
    "\n",
    "\n",
    "    # bayesian model averaging (over policies)\n",
    "    for f in range(Nf):\n",
    "        for i in range(S):\n",
    "            X[f][i] = x[f][:, i, :] @ ext_u[t] #FIXME: might be cause for troubles\n",
    "\n",
    "    \n",
    "    # processing (reaction) time\n",
    "    F_hist[t] = F\n",
    "    Q_hist[t] = Q\n",
    "    H_hist[t] = qu @ F[p] - qu @ (spm_log(qu) - spm_log(pu))\n",
    "\n",
    "\n",
    "    if t < T -1:\n",
    "\n",
    "        Pu = np.zeros(Nu)\n",
    "\n",
    "        for i in range(Np):\n",
    "            Pu[V[t, i, 1]] += ext_u[t][i] # still tiny differences in outcome\n",
    "\n",
    "        Pu = softmax(np.log(Pu) * alpha)\n",
    "        P[t] = Pu\n",
    "\n",
    "        if len(u) > t: # TODO: maybe should be filled with nan instead?\n",
    "            u[t] = u[t] # unecessary assignment\n",
    "        else:\n",
    "            idx = np.argmax(np.random.rand() < np.cumsum(Pu))\n",
    "            u.append(np.unravel_index(idx, [1, Nu]))\n",
    "\n",
    "        pass\n",
    "\n",
    "    #TODO: There's code to check if U is used\n",
    "            \n",
    "# if t == T:\n",
    "#     if T == 1:\n",
    "#         u = np.zeros((T, 1))\n",
    "#         o = None\n",
    "#         s = None\n",
    "#         u = None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import special\n",
    "\n",
    "def betaln(x):\n",
    "    # only compute the gammaln for elements greater than 0\n",
    "    x = x[x > 0]\n",
    "    return special.gammaln(x).sum() - special.gammaln(x.sum())\n",
    "\n",
    "def psi(x):\n",
    "    return special.psi(x) - special.psi(x.sum())\n",
    "\n",
    "\n",
    "# learning - accumulate concentration parameters\n",
    "# X is different \n",
    "# initial hidden states\n",
    "for f in range(Nf):\n",
    "    i = d[f] > 0\n",
    "    # d[f][i] = d[f][i] * omega + X[f][0, i] * eta # TODO: (seems like correct output [(1.0000e+00, 1.2525e-08), (1)])\n",
    "    d[f] = (d[f] * omega + X[f][0] * eta) * i # TODO: (seems like correct output [(1.0000e+00, 1.2525e-08), (1)])\n",
    "    print(X[f][0, i])\n",
    "\n",
    "Fd = np.zeros(Nf)\n",
    "# (negative) free energy of parameters: state specific\n",
    "# compute KL divergence between two dirichlet distributions\n",
    "for f in range(Nf):\n",
    "    Fd[f] = - ( betaln(pD[f]) - betaln(d[f]) - np.sum((pD[f] - d[f]) * psi(d[f] + 1/32))) \n",
    "    # 0.2473\n",
    "    # -0.04099317344047182\n",
    "\n",
    "# simulated dopamine (or cholinergic) responses\n",
    "# wn is different\n",
    "if Np > 1:\n",
    "    dn = 8 * np.gradient(wn) + wn / 8 \n",
    "\n",
    "# Bayesian model averaging of expected hidden states over policies\n",
    "Xn_arr = []\n",
    "Vn_arr = []\n",
    "for f in range(Nf):\n",
    "    Xn = np.zeros((Ni, Ns[f], T, T))\n",
    "    Vn = np.zeros((Ni, Ns[f], T, T))\n",
    "\n",
    "    for t in range(T):\n",
    "        for k in range(Np):\n",
    "            Xn[:, :, :, t] = Xn[:, :, :, t] + xn[f][:, :, :, t, k] * ext_u[t, k]\n",
    "            Vn[:, :, :, t] = Vn[:, :, :, t] + vn[f][:, :, :, t, k] * ext_u[t, k]\n",
    "\n",
    "    Xn_arr.append(Xn)\n",
    "    Vn_arr.append(Vn)\n",
    "\n",
    "# number of belief updates (T)\n",
    "# number of outcomes (O)\n",
    "# probability of action at time point (P)\n",
    "# conditional expectations over policies ((ext_)u) -> R\n",
    "# conditional expectations over N states (x -> Q)\n",
    "# Bayesian model averages over T outcomes (X)\n",
    "# utility (C)\n",
    "# posterior expectations of precision (policy) (w)\n",
    "# simulated neuronal prediction error (Vn)\n",
    "# simulated neuronal encoding of hidden states (Xn)\n",
    "# simulated neuronal encoding of policies (un)\n",
    "# simulated neuronal encoding of precision (wn)\n",
    "# simulated dopamine responses (deconvolved) (dn)\n",
    "# simulated reaction time (seconds) (rt)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
